import azure.functions as func
import logging
import os
import io
import pandas as pd
import joblib
import gc
from azure.storage.blob import BlobServiceClient
from datetime import timedelta
from inference_utils import preprocess_data, predict_next_row

app = func.FunctionApp()

# ì „ì—­ ë³€ìˆ˜ë¡œ ëª¨ë¸ ìºì‹±
_models = None
_scalers = None
_target_columns = None
_feature_columns = None
_model_loaded = False

def load_models_once():
    """ëª¨ë¸ì„ í•œ ë²ˆë§Œ ë¡œë“œí•˜ê³  ì „ì—­ ë³€ìˆ˜ì— ì €ì¥"""
    global _models, _scalers, _target_columns, _feature_columns, _model_loaded
    
    if _model_loaded:
        return _models, _scalers, _target_columns, _feature_columns
    
    try:
        # ì—°ê²° ë¬¸ìì—´ ê°€ì ¸ì˜¤ê¸°
        conn_str = os.environ["AZURE_STORAGE_CONNECTION_STRING"]
        blob_service = BlobServiceClient.from_connection_string(conn_str)
        container_name = "dt026test"

        # ì²­í¬ ë‹¨ìœ„ ë¡œë”© í•¨ìˆ˜
        def load_large_pickle(blob_path, chunk_size=8*1024*1024):  # 8MB ì²­í¬
            try:
                blob_client = blob_service.get_blob_client(container=container_name, blob=blob_path)
                if not blob_client.exists():
                    raise FileNotFoundError(f"Blob '{blob_path}' does not exist")
                
                logging.info(f"ğŸ”„ '{blob_path}' ë¡œë”© ì‹œì‘...")
                
                # ì „ì²´ ìŠ¤íŠ¸ë¦¼ì„ ë©”ëª¨ë¦¬ì— ë¡œë“œ
                stream = io.BytesIO()
                
                # ì²­í¬ ë‹¨ìœ„ë¡œ ë‹¤ìš´ë¡œë“œ
                blob_size = blob_client.get_blob_properties().size
                logging.info(f"ğŸ“ íŒŒì¼ í¬ê¸°: {blob_size / (1024*1024):.2f} MB")
                
                downloaded = 0
                for chunk in blob_client.download_blob().chunks():
                    stream.write(chunk)
                    downloaded += len(chunk)
                    if downloaded % (50*1024*1024) == 0:  # 50MBë§ˆë‹¤ ë¡œê·¸
                        logging.info(f"ğŸ“¥ ë‹¤ìš´ë¡œë“œ ì§„í–‰: {downloaded / (1024*1024):.2f} MB / {blob_size / (1024*1024):.2f} MB")
                
                stream.seek(0)
                logging.info(f"âœ… '{blob_path}' ë‹¤ìš´ë¡œë“œ ì™„ë£Œ")
                
                # joblibë¡œ ë¡œë“œ
                result = joblib.load(stream)
                logging.info(f"âœ… '{blob_path}' ì—­ì§ë ¬í™” ì™„ë£Œ")
                
                # ë©”ëª¨ë¦¬ ì •ë¦¬
                stream.close()
                del stream
                gc.collect()
                
                return result
                
            except Exception as e:
                logging.error(f"âŒ '{blob_path}' ë¡œë”© ì¤‘ ì˜¤ë¥˜: {str(e)}")
                raise

        # ëª¨ë¸ ë° ë¶€ì† íŒŒì¼ ë¡œë”©
        logging.info("ğŸ”„ ëª¨ë¸ íŒŒì¼ë“¤ ë¡œë”© ì‹œì‘...")
        
        _models = load_large_pickle("models/models.pkl")
        logging.info("âœ… ëª¨ë¸ ë¡œë”© ì™„ë£Œ")
        
        _scalers = load_large_pickle("models/scalers.pkl")
        logging.info("âœ… ìŠ¤ì¼€ì¼ëŸ¬ ë¡œë”© ì™„ë£Œ")
        
        _target_columns = load_large_pickle("models/target_columns.pkl")
        logging.info("âœ… íƒ€ê²Ÿ ì»¬ëŸ¼ ë¡œë”© ì™„ë£Œ")
        
        _feature_columns = load_large_pickle("models/feature_columns.pkl")
        logging.info("âœ… í”¼ì²˜ ì»¬ëŸ¼ ë¡œë”© ì™„ë£Œ")
        
        _model_loaded = True
        
        # ë©”ëª¨ë¦¬ ì •ë¦¬
        gc.collect()
        
        logging.info("ğŸ‰ ëª¨ë“  ëª¨ë¸ íŒŒì¼ ë¡œë”© ì™„ë£Œ")
        return _models, _scalers, _target_columns, _feature_columns
        
    except Exception as e:
        logging.error(f"âŒ ëª¨ë¸ ë¡œë”© ì¤‘ ì¹˜ëª…ì  ì˜¤ë¥˜: {str(e)}")
        raise

@app.timer_trigger(schedule="0 */30 * * * *", arg_name="myTimer", run_on_startup=False, use_monitor=False) 
def predictBlobFn(myTimer: func.TimerRequest) -> None:

    if myTimer.past_due:
        logging.info('â° íƒ€ì´ë¨¸ê°€ ì§€ì—°ë˜ì—ˆìŠµë‹ˆë‹¤!')

    logging.info('ğŸš€ Azure Function ì˜ˆì¸¡ ì‹œì‘')

    try:
        # ëª¨ë¸ ë¡œë“œ (ìºì‹± ì‚¬ìš©)
        models, scalers, target_columns, feature_columns = load_models_once()
        
        # ì—°ê²° ë¬¸ìì—´ ê°€ì ¸ì˜¤ê¸°
        conn_str = os.environ["AZURE_STORAGE_CONNECTION_STRING"]
        blob_service = BlobServiceClient.from_connection_string(conn_str)
        container_name = "dt026test"

        # ë°ì´í„° ë¡œë“œ
        logging.info("ğŸ“Š ë°ì´í„° ë¡œë”© ì‹œì‘...")
        blob_data = blob_service.get_blob_client(container=container_name, blob="ML_MAIN_.csv")
        if not blob_data.exists():
            logging.error("âŒ ML_MAIN_.csv íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
            return

        # CSV ë°ì´í„° ë¡œë“œ
        stream = io.BytesIO()
        blob_data.download_blob().readinto(stream)
        stream.seek(0)
        df = pd.read_csv(stream)
        logging.info(f"âœ… ë°ì´í„° ë¡œë”© ì™„ë£Œ: {df.shape}")
        
        # ìŠ¤íŠ¸ë¦¼ ë©”ëª¨ë¦¬ ì •ë¦¬
        stream.close()
        del stream

        # ì „ì²˜ë¦¬ ë° ì˜ˆì¸¡
        logging.info("ğŸ”§ ë°ì´í„° ì „ì²˜ë¦¬ ì‹œì‘...")
        df_processed = preprocess_data(df)
        
        logging.info("ğŸ”® ì˜ˆì¸¡ ì‹œì‘...")
        prediction = predict_next_row(df_processed, models, scalers, target_columns, feature_columns)
        
        # ë©”ëª¨ë¦¬ ì •ë¦¬
        del df, df_processed
        gc.collect()

        # ê²°ê³¼ ì €ì¥
        logging.info("ğŸ’¾ ê²°ê³¼ ì €ì¥ ì‹œì‘...")
        result_stream = io.StringIO()
        prediction.to_csv(result_stream, index=False)
        result_stream.seek(0)

        result_blob = blob_service.get_blob_client(container=container_name, blob="last_prediction.csv")
        result_blob.upload_blob(result_stream.getvalue(), overwrite=True)
        
        # ê²°ê³¼ ìŠ¤íŠ¸ë¦¼ ì •ë¦¬
        result_stream.close()
        del result_stream, prediction
        gc.collect()

        logging.info("ğŸ‰ ì˜ˆì¸¡ ë° ê²°ê³¼ ì €ì¥ ì™„ë£Œ")

    except Exception as e:
        logging.error(f"âŒ ì˜ˆì¸¡ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        import traceback
        logging.error(f"ğŸ“‹ ìƒì„¸ ì˜¤ë¥˜: {traceback.format_exc()}")
        
        # ë©”ëª¨ë¦¬ ì •ë¦¬
        gc.collect()
        
        # ì˜¤ë¥˜ ì‹œ ëª¨ë¸ ìºì‹œ ì´ˆê¸°í™” (ë‹¤ìŒ ì‹¤í–‰ì—ì„œ ì¬ë¡œë“œ)
        global _model_loaded
        _model_loaded = False